{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House price prediction: Advanced, code your own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are going to code up our own linear regression model using 1.) least squares and 2.) gradient descent. We will apply to house price prediction using a subset of the [California house price dataset](https://www.kaggle.com/camnugent/california-housing-prices). Our dataset contains 200 observations for housing blocks in California obtained from the 1990 census. The dataset contains columns:\n",
    "\n",
    "1. `longitude`: A measure of how far west a house is; a higher value is farther west\n",
    "\n",
    "2. `latitude`: A measure of how far north a house is; a higher value is farther north\n",
    "\n",
    "3. `housing_median_age`: Median age of a house within a block; a lower number is a newer building\n",
    "\n",
    "4. `total_rooms`: Total number of rooms within a block\n",
    "\n",
    "5. `total_bedrooms`: Total number of bedrooms within a block\n",
    "\n",
    "6. `population`: Total number of people residing within a block\n",
    "\n",
    "7. `households`: Total number of households, a group of people residing within a home unit, for a block\n",
    "\n",
    "8. `median_income`: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "\n",
    "9. `median_house_value`: Median house value for households within a block (measured in US Dollars)\n",
    "\n",
    "10. `ocean_proximity`: Location of the house w.r.t ocean/sea\n",
    "\n",
    "In this example, we are going to create a regression model to predict `median_house_value` using only `median_income`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the first 200 rows of the file `sample_data/california_housing_train.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    filepath = \"https://raw.githubusercontent.com/lm2612/Tutorials/refs/heads/main/1_supervised_learning_regression/housing_short.csv\"\n",
    "    print(f\"Notebook running in google colab. Using raw github filepath = {filepath}\")\n",
    "\n",
    "else:\n",
    "    filepath = \"./housing_short.csv\"\n",
    "    print(f\"Notebook running locally. Using local filepath = {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict `median_house_value`. This will be our dependent variable, $y$. Pick another variable that you think will be a useful predictor of house value, that we will use as our dependent variable, $x$. First, we should check if these variables appear correlated by plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your choice of variable seem suitable for linear regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "Split the dataset into a suitable training, validation and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = # Use df.iloc[ ... , :] where \"...\" is your choice of indices \n",
    "validation = \n",
    "testing = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a linear regression model to predict median house value from median income using the training set. This is the advanced tutorial, so we will be building our linear regression from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From scratch \n",
    "In this notebook, we will code up linear regression using \n",
    "1. Least squares\n",
    "2. Gradient descent \n",
    "3. Stochastic gradient descent. \n",
    "\n",
    "We will also compare these to the sklearn and pytorch implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and setup\n",
    "Get $(X,y)$ arrays and normalise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming that training set is a pandas dataframe\n",
    "X = training[\"median_income\"]\n",
    "y = training[\"median_house_value\"]\n",
    "\n",
    "X = X.iloc[:, ].values.reshape(-1, 1)      # This reshapes the array so the inputs are the correct size (N, 1)\n",
    "y = y.iloc[:, ].values.reshape(-1, 1)      \n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use StandardScaler to fit and transform your X data so the variables are normalised to zero mean, unit variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our linear regression equation is\n",
    "$$ y = \\beta_0 + \\beta_1 x_1$$\n",
    "or in the matrix formulation: \n",
    "$$\\mathbf{y} = \\beta \\mathbf{X}$$\n",
    "We have $N$ data pairs of inputs and outputs, which we will index with subscript $i$, i.e., ${\\mathbf{X}_i, \\mathbf{y}_i}$ where $i=1, \\cdots, N$. We use this linear model to make predictions for $\\mathbf{y}_i$. The residual error from our linear model is the difference between a prediction and the true data, i.e., $|\\mathbf{y}_i - \\beta \\mathbf{X_i}|$.\n",
    "We want to find $\\beta$ that minimises the residual sum of squares over the dataset, i.e.,\n",
    "$$\\arg \\min_\\beta \\{ \\sum_{i=1}^{N} (\\mathbf{y}_i - \\beta \\mathbf{X}_i)^2 \\}$$ \n",
    "We will call this term inside the curly brackets our Loss function, $L$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimise this, take the gradient with respect to $\\beta$ and set it to zero\n",
    "$$\\frac{dL}{d\\beta}= \\sum_{i=1}^N (-2 \\mathbf{X}_i^T \\mathbf{y}_i + 2 \\beta \\mathbf{X}_i^T \\mathbf{X}_i) = 0 $$\n",
    "$$ \\hat{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\cdot \\mathbf{X}^T \\mathbf{y} $$\n",
    "This is the least squares estimate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $\\beta$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Instead of analytically computing the gradient, we can also use the gradient descent algorithm to estimate $\\hat{\\beta}$.\n",
    "\n",
    "$$ \\beta' = \\beta - \\alpha \\nabla L $$\n",
    "$\\alpha$ is called the learning rate and is usually a small number. We can start with 0.01.\n",
    "\n",
    "1. Start with a guess for $\\hat{\\beta}$\n",
    "2. Compute $\\hat{\\mathbf{y}}=\\hat{\\beta} \\mathbf{X}$. \n",
    "The mean squared error loss is $L=\\frac{1}{N} \\sum_{i=1}^{N}{(\\hat{\\mathbf{y}}_i - \\mathbf{y}_i)^2}=\\frac{1}{N} \\sum_{i=1}^{N}{(\\hat{\\beta} \\mathbf{X}_i - \\mathbf{y}_i)^2}$\n",
    "3. Compute the direction of the gradient of the loss with respect to $\\beta$. This is\n",
    "$$ \\nabla L = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\beta \\mathbf{X}^T_i \\mathbf{X}_i - \\mathbf{X}^T_i \\mathbf{y}_i) = \\frac{1}{N} 2 \\mathbf{X}^T \\cdot (\\hat{\\mathbf{y}} - \\mathbf{y})$$\n",
    "4. Update our estimate of $\\beta$ by taking the step in the direction of reducing the gradient.\n",
    "5. Repeat steps 2-4. until our estimate of $\\beta$ converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code up gradient descent and carry out 100 iterations. Then plot the coefficient, intercept and the loss for these iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your learning rate\n",
    "alpha = \n",
    "\n",
    "# store intercept and coefficients for plotting\n",
    "intercepts = []\n",
    "coefficients = []\n",
    "losses = []\n",
    "\n",
    "\n",
    "# initialise beta_hat randomly\n",
    "beta_hat = np.random.rand(2, 1)\n",
    "\n",
    "for t in range(100):\n",
    "    y_hat =\n",
    "    # compute direction of gradient\n",
    "    d_L =  \n",
    "    \n",
    "    # update\n",
    "    beta_hat = \n",
    "    \n",
    "    # store\n",
    "    intercepts.append(\n",
    "    coefficients.append(\n",
    "    losses.append(\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "axs[0].plot(coefficients)\n",
    "axs[0].set_title(\"Coefficient\")\n",
    "\n",
    "axs[1].plot(intercepts)\n",
    "axs[1].set_title(\"Intercept\")\n",
    "\n",
    "axs[2].plot(losses)\n",
    "axs[2].set_title(\"MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different values for $\\alpha$. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "With a relatively small dataset, we can estimate the gradient over the entire dataset. But when we have larger datasets, this becomes computationally expensive. Stochatic gradient descent is a stochastic approximation of gradient descent optimization. It replaces the true gradient calculated from the entire data set with an estimate of it, calculated from a randomly selected subset of the data.\n",
    "\n",
    "Use a batch size of 32 and carry out gradient descent on one batch at a time. Iterate through the data 100 times (epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "N = X_mat.shape[0]\n",
    "data_inds = np.arange(N)\n",
    "np.random.shuffle(data_inds)\n",
    "# Check the data are shuffled\n",
    "data_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now build your loop over 100 epochs and an inner over the batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "axs[0].plot(coefficients)\n",
    "axs[0].set_title(\"Coefficient\")\n",
    "\n",
    "axs[1].plot(intercepts)\n",
    "axs[1].set_title(\"Intercept\")\n",
    "\n",
    "axs[2].plot(losses)\n",
    "axs[2].set_title(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn solution\n",
    "Compare your results to the sklearn solution `sklearn.linear_model.LinearRegression()`. See https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Create your linear regression model and print the coefficients and intercept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you get the same results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch solution\n",
    "Can you also use a pytorch implementation of linear regression? Explore different learning rates and see how quickly your parameters converge\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a linear layer (1D-> 1D) using torch.nn.Linear(...) \n",
    "linear_layer = \n",
    "# For optimiser, look at the docs for torch.optim.SGD()\n",
    "optimiser = \n",
    "# For the loss function, look at the docs for torch.nn.MSE()\n",
    "loss_function = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the parameters of your neural network. There should be two: one for the coefficient/weight and one for the intercept/bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in linear_layer.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your np arrays into torch tensors using torch.tensor(...)\n",
    "X_torch = torch.tensor(\n",
    "y_torch = torch.tensor("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store parameters for plotting \n",
    "coefficients = []\n",
    "intercepts = []\n",
    "losses = []\n",
    "\n",
    "for iteration in range(100):\n",
    "    optimiser.zero_grad()\n",
    "    pred = linear_layer(X_torch)\n",
    "    loss = loss_function(pred, y_torch)\n",
    "    loss.backward()\n",
    "    \n",
    "    for p, param in enumerate(linear_layer.parameters()):\n",
    "        if p==0:\n",
    "            coefficients.append(param.item())\n",
    "        elif p==1:\n",
    "            intercepts.append(param.item())\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Update optimiser\n",
    "    optimiser.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your parameters after training - they should have converged to the same values as the least squares estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "axs[0].plot(coefficients)\n",
    "axs[0].set_title(\"Coefficient\")\n",
    "\n",
    "axs[1].plot(intercepts)\n",
    "axs[1].set_title(\"Intercept\")\n",
    "\n",
    "axs[2].plot(losses)\n",
    "axs[2].set_title(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
