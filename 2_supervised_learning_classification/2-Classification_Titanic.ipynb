{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/lm2612/Tutorials/blob/main/2_supervised_learning_classification/2-Classification_Titanic.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c259f",
   "metadata": {},
   "source": [
    "## Titanic: Machine learning from disaster\n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "In this tutorial, we will use passenger data to predict who survived the shipwreck and also use our predictive model to answer the question: \"what sorts of people were more likely to survive?\". We will focus on passenger age, gender and socio-economic class). You can read more about the Titanic dataset [here](https://www.kaggle.com/c/titanic/overview).\n",
    "\n",
    "First, import packages and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73896b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    filepath = \"https://raw.githubusercontent.com/lm2612/Tutorials/refs/heads/main/2_supervised_learning_classification/titanic.csv\"\n",
    "    print(f\"Notebook running in google colab. Using raw github filepath = {filepath}\")\n",
    "\n",
    "else:\n",
    "    filepath = \"./titanic.csv\"\n",
    "    print(f\"Notebook running locally. Using local filepath = {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717deb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d726a57",
   "metadata": {},
   "source": [
    "We are interested in the \"Survived\" column, where are two possible outcomes: survived (1) or did not survive (0). We want to build a classifier to predict this outcome. In this tutorial, we are going to compare different classification methods, where we try to determine the factors that influence whether a passenger survived. \n",
    "Specifically, we are going to investigate how the passenger class, age and sex influenced survival.\n",
    "\n",
    "For passenger class, we are going to use dummy variables to represent the three possible states: binary variables which take on the value 0 if not true and 1 if true.\n",
    "\n",
    "Create dummy variables for classes 1 and 2. This implicitly means that the 3rd class will be the base case that we compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152ef29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff78cfa",
   "metadata": {},
   "source": [
    "Create a dummy variable equal to 1 if the passenger was female.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319baf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc67e04c",
   "metadata": {},
   "source": [
    "For age, we are going to split the data up into three segments: (i) those aged 16 or less; (ii) those between 16 and 60; (iii) and those over 60. Create dummy variables for categories (i) and (iii)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4600f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c40c8f6",
   "metadata": {},
   "source": [
    "Clean up the data - drop all variables except for the 'Age', 'Sex', 'Pclass' for our inputs and  'Survived' for our outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6ded9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0e0cf5a",
   "metadata": {},
   "source": [
    "Split the data into training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb30d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85a98800",
   "metadata": {},
   "source": [
    "Set up your X and y variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dd29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8e4a6a9",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "We will use the `sklearn.linear_model.LogisticRegression`. Read the docs here: https://scikit-learn.org/0.16/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f314a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set up and fit the logistic regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d103e33",
   "metadata": {},
   "source": [
    "What variable is given the most relevance for a prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c91ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0925fab9",
   "metadata": {},
   "source": [
    "## Validating our classification model\n",
    "Predict on the validation dataset and estimate accuracy using one of the classification metrics from [here](https://scikit-learn.org/1.5/modules/model_evaluation.html#classification-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7eacd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4707767",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "Now build decision tree using `sklearn.tree.DecisionTreeClassifier` using the docs here: https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier. Use the entropy criterion for splitting we discussed during lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12654fe",
   "metadata": {},
   "source": [
    "How does the decision tree compare against the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d349c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f24ae92e",
   "metadata": {},
   "source": [
    "## Visualising how good the model is\n",
    "We can look at how often the models produce false positives and false negatives using a [confusion matrix](https://scikit-learn.org/1.5/auto_examples/model_selection/plot_confusion_matrix.html). It shows:\n",
    "\n",
    "\n",
    "```\n",
    "                           | Predicted Negatives (0) | Predicted Positives (1) |\n",
    "--------------------------------------------------------------------------------\n",
    "True Negatives (0)         | True Negatives          | False Positives         |\n",
    "True Positives (1)         | False Negatives         | True Positives          |\n",
    "```\n",
    "\n",
    "We want higher values along the upper left to lower right diagonal (more true negatives / true positives) and lower values in the opposite diagonal (fewer false negatives / false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aeb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Compare the confusion matrices on the validation data for the regression and tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e28f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ab16e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
