{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/lm2612/Tutorials/blob/main/3_deeplearning/3-DeepLearning_HousePrice.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: House price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Tuesday, we used linear regression to predict house prices of the [California house price dataset](https://www.kaggle.com/camnugent/california-housing-prices). Our dataset contains 200 observations for housing blocks in California obtained from the 1990 census. The dataset contains columns:\n",
    "\n",
    "1. `longitude`: A measure of how far west a house is; a higher value is farther west\n",
    "\n",
    "2. `latitude`: A measure of how far north a house is; a higher value is farther north\n",
    "\n",
    "3. `housing_median_age`: Median age of a house within a block; a lower number is a newer building\n",
    "\n",
    "4. `total_rooms`: Total number of rooms within a block\n",
    "\n",
    "5. `total_bedrooms`: Total number of bedrooms within a block\n",
    "\n",
    "6. `population`: Total number of people residing within a block\n",
    "\n",
    "7. `households`: Total number of households, a group of people residing within a home unit, for a block\n",
    "\n",
    "8. `median_income`: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "\n",
    "9. `median_house_value`: Median house value for households within a block (measured in US Dollars)\n",
    "\n",
    "10. `ocean_proximity`: Location of the house w.r.t ocean/sea\n",
    "\n",
    "Previously, we used intuition to guess what input variables would be suitable predictors. In this example, we are going to  use all the variables available and to predict `median_house_value`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must be on google colab for this tutorial - otherwise you will not be able to open the dataset. First, import the needed modules, we will be using `torch` for building neural networks. On Tuesday we used a smaller subset of the full dataset. Load the file `sample_data/california_housing_train.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-114.31</td>\n",
       "      <td>34.19</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5612.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>1.4936</td>\n",
       "      <td>66900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-114.47</td>\n",
       "      <td>34.40</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7650.0</td>\n",
       "      <td>1901.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.8200</td>\n",
       "      <td>80100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-114.56</td>\n",
       "      <td>33.69</td>\n",
       "      <td>17.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1.6509</td>\n",
       "      <td>85700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-114.57</td>\n",
       "      <td>33.64</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>3.1917</td>\n",
       "      <td>73400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-114.57</td>\n",
       "      <td>33.57</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>1.9250</td>\n",
       "      <td>65500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -114.31     34.19                15.0       5612.0          1283.0   \n",
       "1    -114.47     34.40                19.0       7650.0          1901.0   \n",
       "2    -114.56     33.69                17.0        720.0           174.0   \n",
       "3    -114.57     33.64                14.0       1501.0           337.0   \n",
       "4    -114.57     33.57                20.0       1454.0           326.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0      1015.0       472.0         1.4936             66900.0  \n",
       "1      1129.0       463.0         1.8200             80100.0  \n",
       "2       333.0       117.0         1.6509             85700.0  \n",
       "3       515.0       226.0         3.1917             73400.0  \n",
       "4       624.0       262.0         1.9250             65500.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "df = pd.read_csv(\"sample_data/california_housing_train.csv\")\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the dataset is much larger than Tuesday! Clean up the data and split it into training and validation. Note, we don't need to set aside test data as we will use the test data in `sample_data/california_housing_test.csv`. Remember the data is ordered, you may want to shuffle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nans\n",
    "df = \n",
    "\n",
    "# Shuffle the data\n",
    "from sklearn.utils import shuffle\n",
    "df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 80% training, 20% validation\n",
    "training = df.iloc[\n",
    "validation = df.iloc["
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get your X and y variables. We will use all of the predictors for X and scale them to zero mean unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = training[\"median_house_value\"].values.reshape(-1, 1)\n",
    "X = training[['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "       'total_bedrooms', 'population', 'households', 'median_income']].values\n",
    "X.shape, y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use StandardScaler to fit and transform your variables\n",
    "scaler_X = \n",
    "X_scaled = \n",
    "\n",
    "scaler_y =\n",
    "y_scaled = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create torch tensors ready for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y = torch.tensor(y_scaled, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple neural network. It can include as many or as few layers as you like. We will start simple with a 3-layer dense neural network, where we have an input layer, and 2 hidden layers with 16 nodes. For the input layer, we need to tell pytorch the number of input features (9). Then we can choose the number of hidden nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, n_features=8, n_targets=1):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_targets = n_targets\n",
    "        \n",
    "        self.layer_input = torch.nn.Linear(self.n_features, 16)         # Input layer: n_features -> 16\n",
    "        self.layer_hidden1 = torch.nn.Linear(16, 16)                    # Hidden layer 1: 16 -> 16\n",
    "        self.layer_hidden2 = torch.nn.Linear(16, self.n_targets)        # Hidden layer 2: 16 -> n_targets\n",
    "\n",
    "        self.activation_function = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Input layer\n",
    "        output = self.layer_input(X)\n",
    "        output = self.activation_function(output)\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        output = self.layer_hidden1(output)\n",
    "        output = self.activation_function(output)\n",
    "\n",
    "        # Hidden layer 2\n",
    "        output = self.layer_hidden2(output)\n",
    "        # Notice we don't use the activation function here - why not? when would we use it?\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check our network on one batch\n",
    "Let's test our neural network on a small batch of data, before we start our training loop. Create an instance of SimpleNet and test it on a batch size of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of SimpleNet \n",
    "my_network = SimpleNet(n_features=8, n_targets=1)\n",
    "\n",
    "# Get the first batch\n",
    "X_batch =\n",
    "y_batch = \n",
    "\n",
    "# Test if we can call my_network without any errors\n",
    "pred_batch = my_network(X_batch)\n",
    "\n",
    "# Check the output is the correct size\n",
    "print(pred_batch.shape)\n",
    "assert(pred_batch.shape == y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up loss function and optimisation algorithms\n",
    "\n",
    "Decide on a suitable loss function. \n",
    "\n",
    "We will use Mean Squared Error (MSE). It is always good to check you can properly call this on your first batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "loss_function(pred_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the optimiser and pass our network parameters to it. We will use the Adam optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(params = my_network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "We are ready to start our training loop. We could manually iterate through the data using indices, e.g., `X[0:64, :], y[0:64, :], ...`. But pytorch simplifies this for us with some useful functions, including mini-batching and shuffling - this will become essential when we move to large datasets. This is done in two steps, both of which are highly customisable. \n",
    "First, we create a `Dataset` which contains all of our data (you can also include any relevant pre-processing functions inside the Dataset). For regression, we provide our inputs and outputs. \n",
    "Then, we use a `DataLoader` that allows us to iterate through minibatches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                         shuffle=True,\n",
    "                                         batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check we can iterate through the dataloader\n",
    "X_batch, y_batch = next(iter(dataloader))\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for X_batch, y_batch in dataloader:\n",
    "    optimiser.zero_grad()\n",
    "    pred_batch = my_network(X_batch)\n",
    "    loss = loss_function(pred_batch, y_batch)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update optimiser\n",
    "    optimiser.step()\n",
    "\n",
    "    # Add MSE losses to our list for plotting\n",
    "    losses.append(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the losses for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check validation dataset\n",
    "Predict using the validation dataset and compare to the true validation dataset. Check our loss function (MSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your X_validation and y_validation variables. \n",
    "y_validation = \n",
    "X_validation = \n",
    "\n",
    "# Scale them and create them as torch tensors (as we did for training)\n",
    "\n",
    "X_validation_scaled = \n",
    "y_validation_scaled = \n",
    "\n",
    "X_validation = torch.tensor(\n",
    "y_validation = torch.tensor(\n",
    "\n",
    "# Create a torch dataset in the same way did before\n",
    "dataset_validation = \n",
    "\n",
    "# And finally, create a validation dataloader with a batch size of 64 in the same way we did before\n",
    "dataloader_validation = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network on the validation dataset! Note, we need to put the network in \"evaluation\" mode first  \n",
    "my_network.eval()       # Puts network into evaluation mode\n",
    "pred_validation = \n",
    "# Check the loss\n",
    "loss =\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the predicted against the truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it compare to your linear regression models on Tuesday? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train over multiple epochs\n",
    "\n",
    "Now you have gone through one full iteration of the data, train over multiple epochs and make sure you go through the validation dataset each epoch. Keep track of the training and validation losses averaged over each epoch separately and plot these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = []\n",
    "validation_losses = []\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set up training\n",
    "    my_network.train()\n",
    "    training_loss = 0\n",
    "    \n",
    "    # TRAINING LOOP\n",
    "    # This will look the similar our previous single iteration\n",
    "    for ...\n",
    "    \n",
    "    # Sum the training loss at each iteration and store the mean training loss at the end of the epoch   \n",
    "    training_losses.append(training_loss)\n",
    "\n",
    "    # Set up validation\n",
    "    my_network.eval()\n",
    "    validation_loss = 0\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    # This will look similar to your training loop, but remember you do not need to do the optimise step\n",
    "    for ...\n",
    "    \n",
    "    # Add MSE losses to our list for plotting\n",
    "    validation_losses.append(validation_loss)\n",
    "    \n",
    "    # After every 10 epochs print mean losses\n",
    "    if epoch%10 ==0:\n",
    "        print(f\"After epoch {epoch}: Training loss={training_loss:.2f}, validation loss={validation_loss:.2f}\")\n",
    "              \n",
    "print(f\"At end of training: Training loss={training_loss:.2f}, validation loss={validation_loss:.2f}\")\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the training and validation loss curves. How do they differ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "Keep training your network and look for signs of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Exploring different choices\n",
    "Play around with different versions of the network. For example, try:\n",
    "* More or fewer layers.\n",
    "* More of fewer hidden nodes.\n",
    "* Different choice of [activation functions](https://pytorch.org/docs/main/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "* Different choice of [loss functions](https://pytorch.org/docs/main/nn.html#loss-functions)\n",
    "* Different choice of [optimiser](https://pytorch.org/docs/main/optim.html#algorithms)\n",
    "\n",
    "Selecting these choices for your problem is an optimisation problem in itself. This is often called hyperparameter selection. There is no best approach - usually people would manually search through some different options until we have the best results. Finding quicker methods for \"hyperparameter optimisation\" is a research field in itself!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More layers\n",
    "Add one layer with 32 hidden nodes and see if it you get a better performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Other network parameters\n",
    "Explore the following options:\n",
    "* add BatchNorm \n",
    "* replace the ReLU activation function with the Sigmoid activation function\n",
    "* remove a hidden layer\n",
    "* try a different optimisation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Select your best network and apply it to the test data and calculate the RMSE. Don't forget your outputs are scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open test data\n",
    "testing = pd.read_csv(\"sample_data/california_housing_test.csv\")\n",
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Linear Regression\n",
    "Compare your code the linear regression results you got on Tuesday. Note you will need to re-run your linear regression on this new test dataset. You may also want to re-train on this larger training dataset for a fair comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
